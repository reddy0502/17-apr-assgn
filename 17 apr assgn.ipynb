{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70170bf-678a-487f-8927-a6a19c731dac",
   "metadata": {},
   "source": [
    "1ans:\n",
    "\n",
    "Gradient Boosting Regression (GBR) is a popular variant of gradient boosting that is used for regression tasks. Like other boosting algorithms, GBR builds an ensemble of weak regressors to create a strong predictor that can generalize well on unseen data. However, unlike other boosting algorithms that update the weights of the training examples, GBR updates the residual errors of the previous model to iteratively improve the model's predictions.\n",
    "\n",
    "2ans:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(0)\n",
    "n_samples = 100\n",
    "x = np.random.rand(n_samples)\n",
    "y = np.sin(2*np.pi*x) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "split = int(n_samples * 0.8)\n",
    "x_train, y_train = x[:split], y[:split]\n",
    "x_test, y_test = x[split:], y[split:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f703de-2026-4e78-aa7e-fedc7e6edc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "3ans:\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "gb = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(gb, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(x_train.reshape(-1, 1), y_train)\n",
    "print(\"Best parameters found by grid search:\", grid_search.best_params_)\n",
    "\n",
    "# Define the parameter distribution for random search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1.0],\n",
    "    'max_depth': [1, 2, 3, 4],\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "gb = GradientBoostingRegressor()\n",
    "random_search = RandomizedSearchCV(gb, param_distributions=param_dist, n_iter=10, cv=5)\n",
    "random_search.fit(x_train.reshape(-1, 1), y_train)\n",
    "print(\"Best parameters found by random search:\", random_search.best_params_)\n",
    "\n",
    "# Train the gradient boosting regressor with the best hyperparameters\n",
    "gb = GradientBoostingRegressor(**grid_search.best_params_)\n",
    "gb.fit(x_train.reshape(-1, 1), y_train)\n",
    "y_pred = gb.predict(x_test.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb42c069-82d5-4d80-a8c4-439211385f0c",
   "metadata": {},
   "source": [
    "4ans:\n",
    "\n",
    "\n",
    "In Gradient Boosting, a weak learner is a model that performs only slightly better than random guessing on the task at hand. It is typically a simple model, such as a decision tree with a small number of nodes, that can be trained quickly and easily. The idea behind Gradient Boosting is to combine multiple weak learners to form a strong learner that performs well on the task at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a2804-0240-462f-b146-bdca2d403610",
   "metadata": {},
   "source": [
    "5ans:\n",
    "\n",
    "The intuition behind this process is that the first weak learner makes predictions that are only slightly better than random guessing. The second weak learner is then trained to correct the errors of the first learner, and so on. By iteratively fitting weak learners to the residual errors, Gradient Boosting is able to form a strong learner that can capture complex relationships between the input features and the target variable.\n",
    "\n",
    "In each iteration, the algorithm uses gradient descent to find the best direction to adjust the predictions of the current weak learner in order to minimize the residual errors. This is where the \"Gradient\" in Gradient Boosting comes from. By adjusting the predictions in the direction of the negative gradient of the loss function, the algorithm is able to make the largest possible reduction in the residual errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d042bf-d13d-44b8-acad-76c7830e1efa",
   "metadata": {},
   "source": [
    "6ans:\n",
    "\n",
    "Initialize the ensemble with a weak learner, such as a decision tree with only one split.\n",
    "\n",
    "Train the weak learner on the training data and use it to make predictions.\n",
    "\n",
    "Calculate the residual errors between the predicted values and the true values.\n",
    "\n",
    "Train a new weak learner on the residual errors.\n",
    "\n",
    "Add the predictions of the new learner to the predictions of the previous learners and calculate the new residual errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d472a4b-d055-4e79-9822-038760ff8e7e",
   "metadata": {},
   "source": [
    "7ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
